{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06dba788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from rdkit import Chem\n",
    "from dgllife.model.model_zoo.dgmg import MoleculeEnv\n",
    "from dgl.data.utils import download, _get_dgl_url\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f96b0e0",
   "metadata": {},
   "source": [
    "## args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5a93624",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'seed': 0,\n",
    "    'warmup_epochs': 10,\n",
    "    'dataset': 'ChEMBL',\n",
    "    'order': 'random',\n",
    "    'train_file': None,\n",
    "    'val_file': None,\n",
    "    'log_dir': 'training_results',\n",
    "    \n",
    "    'num_processes': 8,\n",
    "    'master_ip': '127.0.0.1',\n",
    "    'master_port': '12345',\n",
    "    \n",
    "    # function configure(args)\n",
    "    'node_hidden_size': 128,\n",
    "    'num_propagation_rounds': 2,\n",
    "    'lr': 1e-4,\n",
    "    'dropout': 0.2,\n",
    "    'nepochs': 400,\n",
    "    'batch_size': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8858feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "def get_date_postfix():\n",
    "    \"\"\"Get a data based postfix for directory name.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    post_fix : str\n",
    "    \"\"\"\n",
    "    dt = datetime.datetime.now()\n",
    "    post_fix = '{}_{:02d}-{:02d}-{:02d}'.format(\n",
    "        dt.date(), dt.hour, dt.minute, dt.second)\n",
    "    \n",
    "    return post_fix\n",
    "\n",
    "\n",
    "def setup_log_dir(args):\n",
    "    \"\"\"Name and create directory fot logging.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    log_dir : str\n",
    "        Path for logging directory\n",
    "    \"\"\"\n",
    "    data_postfix = get_date_postfix()\n",
    "    log_dir = os.path.join(\n",
    "        args['log_dir'],\n",
    "        '{}_{}_{}'.format(\n",
    "            args['dataset'],\n",
    "            args['order'],\n",
    "            data_postfix)\n",
    "        )\n",
    "    mkdir_p(log_dir)\n",
    "    return log_dir\n",
    "\n",
    "def mkdir_p(path, log=True):\n",
    "    \"\"\"Create a directory for the specified path.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path name\n",
    "    log : bool\n",
    "        Whether to print result for directory creation\n",
    "    \"\"\"\n",
    "    import errno\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "        if log:\n",
    "            print('Create directory {}'.format(path))\n",
    "    except OSError as exc:\n",
    "        if exc.errno == errno.EEXIST and ps.path.isdir(path) and log:\n",
    "            print('Directory {} already exists.'.format(path))\n",
    "        else:\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40c39879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create directory training_results/ChEMBL_random_2023-12-08_11-35-20\n"
     ]
    }
   ],
   "source": [
    "log_dir = setup_log_dir(args)\n",
    "args['log_dir'] = log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "660ad416",
   "metadata": {},
   "outputs": [],
   "source": [
    "args['checkpoint_dir'] = os.path.join(log_dir, 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c7b004",
   "metadata": {},
   "source": [
    "## set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e455717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    '''Fix random seed for reproducible results.\n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    seed: int\n",
    "        Random seed to use\n",
    "    '''\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04988628",
   "metadata": {},
   "source": [
    "Paramters of `torch.set_num_threads()`:\n",
    "- int, sets the number of threads used for intraop parallelism of CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa263653",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45591e16",
   "metadata": {},
   "source": [
    "## Setup dataset and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a6bd4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:160: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:160: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/var/folders/_c/34hk0_0158s0rgbkt3p41wxw0000gn/T/ipykernel_19237/4140037834.py:160: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if self.order is 'random':\n"
     ]
    }
   ],
   "source": [
    "class MoleculeDataset(object):\n",
    "    \"\"\"Initialize and split the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : str\n",
    "        Dataset name\n",
    "    order : None or str\n",
    "        Order to extract a decision sequence for generating a molecule. Default to be None.\n",
    "    modes : None or list\n",
    "        List of subsets to use, which can contain 'train', 'val', corresponding to\n",
    "        training and validation. Default to be None.\n",
    "    subset_id : int\n",
    "        With multiprocess training, we partition the training set into multiple subsets and\n",
    "        each process will use one subset only. This subset_id corresponds to subprocess id.\n",
    "    n_subsets : int\n",
    "        With multiprocess training, this corresponds to the number of total subprocesses.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, order=None, modes=None, subset_id=0, n_subsets=1):\n",
    "        super(MoleculeDataset, self).__init__()\n",
    "\n",
    "        if modes is None:\n",
    "            modes = []\n",
    "        else:\n",
    "            assert order is not None, 'An order should be specified for extracting ' \\\n",
    "                                      'decision sequences.'\n",
    "\n",
    "        assert order in ['random', 'canonical', None], \\\n",
    "            \"Unexpected order option to get sequences of graph generation decisions\"\n",
    "        assert len(set(modes) - {'train', 'val'}) == 0, \\\n",
    "            \"modes should be a list, representing a subset of ['train', 'val']\"\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.order = order\n",
    "        self.modes = modes\n",
    "        self.subset_id = subset_id\n",
    "        self.n_subsets = n_subsets\n",
    "        self._setup()\n",
    "\n",
    "    def collate(self, samples):\n",
    "        \"\"\"PyTorch's approach to batch multiple samples.\n",
    "\n",
    "        For auto-regressive generative models, we process one sample at a time.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        samples : list\n",
    "            A list of length 1 that consists of decision sequence to generate a molecule.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            List of 2-tuples, a decision sequence to generate a molecule\n",
    "        \"\"\"\n",
    "        assert len(samples) == 1\n",
    "        return samples[0]\n",
    "\n",
    "    def _create_a_subset(self, smiles):\n",
    "        \"\"\"Create a dataset from a subset of smiles.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        smiles : list of str\n",
    "            List of molecules in SMILES format\n",
    "        \"\"\"\n",
    "        # We evenly divide the smiles into multiple susbets with multiprocess\n",
    "        subset_size = len(smiles) // self.n_subsets\n",
    "        return Subset(smiles[self.subset_id * subset_size: (self.subset_id + 1) * subset_size],\n",
    "                      self.order, self.env)\n",
    "\n",
    "    def _setup(self):\n",
    "        \"\"\"\n",
    "        1. Instantiate an MDP environment for molecule generation\n",
    "        2. Download the dataset, which is a file of SMILES\n",
    "        3. Create subsets for training and validation\n",
    "        \"\"\"\n",
    "        if self.dataset == 'ChEMBL':\n",
    "            # For new datasets, get_atom_and_bond_types can be used to\n",
    "            # identify the atom and bond types in them.\n",
    "            self.atom_types = ['O', 'Cl', 'C', 'S', 'F', 'Br', 'N']\n",
    "            self.bond_types = [Chem.rdchem.BondType.SINGLE,\n",
    "                               Chem.rdchem.BondType.DOUBLE,\n",
    "                               Chem.rdchem.BondType.TRIPLE]\n",
    "\n",
    "        elif self.dataset == 'ZINC':\n",
    "            self.atom_types = ['Br', 'S', 'C', 'P', 'N', 'O', 'F', 'Cl', 'I']\n",
    "            self.bond_types = [Chem.rdchem.BondType.SINGLE,\n",
    "                               Chem.rdchem.BondType.DOUBLE,\n",
    "                               Chem.rdchem.BondType.TRIPLE]\n",
    "\n",
    "        else:\n",
    "            path_to_atom_and_bond_types = '_'.join([self.dataset, 'atom_and_bond_types.pkl'])\n",
    "            with open(path_to_atom_and_bond_types, 'rb') as f:\n",
    "                type_info = pickle.load(f)\n",
    "            self.atom_types = type_info['atom_types']\n",
    "            self.bond_types = type_info['bond_types']\n",
    "        self.env = MoleculeEnv(self.atom_types, self.bond_types)\n",
    "\n",
    "        dataset_prefix = self._dataset_prefix()\n",
    "\n",
    "        if 'train' in self.modes:\n",
    "            fname = '_'.join([dataset_prefix, 'train.txt'])\n",
    "            download_data(self.dataset, fname)\n",
    "            smiles = load_smiles_from_file(fname)\n",
    "            self.train_set = self._create_a_subset(smiles)\n",
    "\n",
    "        if 'val' in self.modes:\n",
    "            fname = '_'.join([dataset_prefix, 'val.txt'])\n",
    "            download_data(self.dataset, fname)\n",
    "            smiles = load_smiles_from_file(fname)\n",
    "            # We evenly divide the smiles into multiple susbets with multiprocess\n",
    "            self.val_set = self._create_a_subset(smiles)\n",
    "\n",
    "    def _dataset_prefix(self):\n",
    "        \"\"\"Get the prefix for the data files of supported datasets.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            Prefix for dataset file name\n",
    "        \"\"\"\n",
    "        return '_'.join([self.dataset, 'DGMG'])\n",
    "\n",
    "class Subset(Dataset):\n",
    "    \"\"\"A set of molecules which can be used for training, validation, test.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    smiles : list\n",
    "        List of SMILES for the dataset\n",
    "    order : str\n",
    "        Specifies how decision sequences for molecule generation\n",
    "        are obtained, can be either \"random\" or \"canonical\"\n",
    "    env : MoleculeEnv object\n",
    "        MDP environment for generating molecules\n",
    "    \"\"\"\n",
    "    def __init__(self, smiles, order, env):\n",
    "        super(Subset, self).__init__()\n",
    "        self.smiles = smiles\n",
    "        self.order = order\n",
    "        self.env = env\n",
    "        self._setup()\n",
    "\n",
    "    def _setup(self):\n",
    "        \"\"\"Convert SMILES into rdkit molecule objects.\n",
    "\n",
    "        Decision sequences are extracted if we use a fixed order.\n",
    "        \"\"\"\n",
    "        smiles_ = []\n",
    "        mols = []\n",
    "        for s in self.smiles:\n",
    "            m = smiles_to_standard_mol(s)\n",
    "            if m is None:\n",
    "                continue\n",
    "            smiles_.append(s)\n",
    "            mols.append(m)\n",
    "        self.smiles = smiles_\n",
    "        self.mols = mols\n",
    "\n",
    "        if self.order is 'random':\n",
    "            return\n",
    "\n",
    "        self.decisions = []\n",
    "        for m in self.mols:\n",
    "            self.decisions.append(\n",
    "                self.env.get_decision_sequence(m, list(range(m.GetNumAtoms())))\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Get number of molecules in the dataset.\"\"\"\n",
    "        return len(self.mols)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \"\"\"Get the decision sequence for generating the molecule indexed by item.\"\"\"\n",
    "        if self.order == 'canonical':\n",
    "            return self.decisions[item]\n",
    "        else:\n",
    "            m = self.mols[item]\n",
    "            nodes = list(range(m.GetNumAtoms()))\n",
    "            random.shuffle(nodes)\n",
    "            return self.env.get_decision_sequence(m, nodes)\n",
    "\n",
    "def download_data(dataset, fname):\n",
    "    \"\"\"Download dataset if built-in support exists\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : str\n",
    "        Dataset name\n",
    "    fname : str\n",
    "        Name of dataset file\n",
    "    \"\"\"\n",
    "    if dataset not in ['ChEMBL', 'ZINC']:\n",
    "        return\n",
    "    \n",
    "    data_path = fname\n",
    "    download(_get_dgl_url(os.path.join('dataset', fname)), path=data_path)\n",
    "\n",
    "    \n",
    "def load_smiles_from_file(f_name):\n",
    "    \"\"\"Load dataset into a list of SMILES\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f_name : str\n",
    "        Path to a file of molecules, where each line of the file\n",
    "        is a molecule in SMILES format.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    smiles : list of str\n",
    "        List of molecules as SMILES\n",
    "    \"\"\"\n",
    "    with open(f_name, 'r') as f:\n",
    "        smiles = f.read().splitlines()\n",
    "    return smiles\n",
    "\n",
    "def smiles_to_standard_mol(s):\n",
    "    \"\"\"Convert SMILES to a standard molecule.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    s : str\n",
    "        SMILES\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Chem.rdchem.Mol\n",
    "        Standardized molecule\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(s)\n",
    "    return standardize_mol(mol)\n",
    "\n",
    "def standardize_mol(mol):\n",
    "    \"\"\"Standardize molecule to avoid false novel molecule.\n",
    "\n",
    "    Kekulize and deprotonate molecules to avoid false novel molecules.\n",
    "\n",
    "    In addition to deprotonation, we also kekulize molecules to avoid\n",
    "    explicit Hs in the SMILES. Otherwise we will get false novel molecules\n",
    "    as well. For example, DGMG can only generate\n",
    "    O=S(=O)(NC1=CC=CC(C(F)(F)F)=C1)C1=CNC=N1\n",
    "    from\n",
    "    O=S(=O)(Nc1cccc(C(F)(F)F)c1)c1c[nH]cn1.\n",
    "\n",
    "    One downside is that we remove all explicit aromatic rings and to\n",
    "    explicitly predict aromatic bond might make the learning easier for\n",
    "    the model.\n",
    "    \"\"\"\n",
    "    reactions = initialize_neuralization_reactions()\n",
    "    Chem.Kekulize(mol, clearAromaticFlags=True)\n",
    "    mol = neutralize_charges(mol, reactions)\n",
    "    return mol\n",
    "\n",
    "def initialize_neuralization_reactions():\n",
    "    \"\"\"Reference neuralization reactions\n",
    "\n",
    "    Code adapted from RDKit Cookbook, by Hans de Winter.\n",
    "    \"\"\"\n",
    "    patts = (\n",
    "        # Imidazoles\n",
    "        ('[n+;H]', 'n'),\n",
    "        # Amines\n",
    "        ('[N+;!H0]', 'N'),\n",
    "        # Carboxylic acids and alcohols\n",
    "        ('[$([O-]);!$([O-][#7])]', 'O'),\n",
    "        # Thiols\n",
    "        ('[S-;X1]', 'S'),\n",
    "        # Sulfonamides\n",
    "        ('[$([N-;X2]S(=O)=O)]', 'N'),\n",
    "        # Enamines\n",
    "        ('[$([N-;X2][C,N]=C)]', 'N'),\n",
    "        # Tetrazoles\n",
    "        ('[n-]', '[n]'),\n",
    "        # Sulfoxides\n",
    "        ('[$([S-]=O)]', 'S'),\n",
    "        # Amides\n",
    "        ('[$([N-]C=O)]', 'N'),\n",
    "    )\n",
    "    return [(Chem.MolFromSmarts(x), Chem.MolFromSmiles(y, False)) for x, y in patts]\n",
    "\n",
    "def neutralize_charges(mol, reactions=None):\n",
    "    \"\"\"Deprotonation for molecules.\n",
    "\n",
    "    Code adapted from RDKit Cookbook, by Hans de Winter.\n",
    "\n",
    "    DGMG currently cannot generate protonated molecules.\n",
    "    For example, it can only generate\n",
    "    CC(C)(C)CC1CCC[NH+]1Cc1nnc(-c2ccccc2F)o1\n",
    "    from\n",
    "    CC(C)(C)CC1CCCN1Cc1nnc(-c2ccccc2F)o1\n",
    "    even with correct decisions.\n",
    "\n",
    "    Deprotonation is therefore an important step to avoid\n",
    "    false novel molecules.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mol : Chem.rdchem.Mol\n",
    "    reactions : list of 2-tuples\n",
    "        Rules for deprotonation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mol : Chem.rdchem.Mol\n",
    "        Deprotonated molecule\n",
    "    \"\"\"\n",
    "    if reactions is None:\n",
    "        reactions = initialize_neuralization_reactions()\n",
    "    for i, (reactant, product) in enumerate(reactions):\n",
    "        while mol.HasSubstructMatch(reactant):\n",
    "            rms = AllChem.ReplaceSubstructs(mol, reactant, product)\n",
    "            mol = rms[0]\n",
    "    return mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d52e8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ChEMBL_DGMG_train.txt from https://data.dgl.ai/dataset/ChEMBL_DGMG_train.txt...\n",
      "Downloading ChEMBL_DGMG_val.txt from https://data.dgl.ai/dataset/ChEMBL_DGMG_val.txt...\n"
     ]
    }
   ],
   "source": [
    "rank = 0\n",
    "\n",
    "dataset = MoleculeDataset(args['dataset'], args['order'], ['train', 'val'],\n",
    "                          subset_id=rank, n_subsets=args['num_processes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b74a8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset.train_set, batch_size=args['batch_size'],\n",
    "                              shuffle=True, collate_fn=dataset.collate)\n",
    "val_loader = DataLoader(dataset.val_set, batch_size=args['batch_size'],\n",
    "                            shuffle=True, collate_fn=dataset.collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f108cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed1cd98a",
   "metadata": {},
   "source": [
    "## Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ab658c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgllife.model import DGMG\n",
    "\n",
    "model = DGMG(atom_types = dataset.atom_types,\n",
    "             bond_types = dataset.bond_types,\n",
    "             node_hidden_size=args['node_hidden_size'],\n",
    "             num_prop_rounds=args['num_propagation_rounds'],\n",
    "             dropout=args['dropout'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a730ddc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DGMG(\n",
       "  (graph_embed): GraphEmbed(\n",
       "    (node_gating): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=1, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "    (node_to_graph): Linear(in_features=128, out_features=256, bias=True)\n",
       "  )\n",
       "  (graph_prop): GraphProp(\n",
       "    (message_funcs): ModuleList(\n",
       "      (0): Linear(in_features=259, out_features=256, bias=True)\n",
       "      (1): Linear(in_features=259, out_features=256, bias=True)\n",
       "    )\n",
       "    (node_update_funcs): ModuleList(\n",
       "      (0): GRUCell(256, 128)\n",
       "      (1): GRUCell(256, 128)\n",
       "    )\n",
       "  )\n",
       "  (add_node_agent): AddNode(\n",
       "    (add_node): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=256, out_features=8, bias=True)\n",
       "    )\n",
       "    (node_type_embed): Embedding(7, 128)\n",
       "    (initialize_hv): Linear(in_features=384, out_features=128, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (add_edge_agent): AddEdge(\n",
       "    (add_edge): Sequential(\n",
       "      (0): Linear(in_features=384, out_features=384, bias=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=384, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (choose_dest_agent): ChooseDestAndUpdate(\n",
       "    (choose_dest): Sequential(\n",
       "      (0): Linear(in_features=259, out_features=259, bias=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=259, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "692b5897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "class Optimizer(nn.Module):\n",
    "    \"\"\"Wrapper for optimization\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : float\n",
    "        Initial learning rate\n",
    "    optimizer\n",
    "        model optimizer\n",
    "    \"\"\"\n",
    "    def __init__(self, lr, optimizer):\n",
    "        super(Optimizer, self).__init__()\n",
    "        self.lr = lr\n",
    "        self.optimizer = optimizer\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def backward_and_step(self, loss):\n",
    "        \"\"\"Backward and update model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        loss : torch.tensor consisting of a float only\n",
    "        \"\"\"\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self._reset()\n",
    "\n",
    "    def decay_lr(self, decay_rate=0.99):\n",
    "        \"\"\"Decay learning rate.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        decay_rate : float\n",
    "            Multiply the current learning rate by the decay_rate\n",
    "        \"\"\"\n",
    "        self.lr *= decay_rate\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = self.lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24d95ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Optimizer(args['lr'], Adam(model.parameters(), lr=args['lr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da1a02ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_prob = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547a8a50",
   "metadata": {},
   "source": [
    "\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53b11522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def evaluate(epoch, model, data_loader, printer):\n",
    "    model.eval()\n",
    "    batch_size = data_loader.batch_size\n",
    "    total_log_prob = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(data_loader):\n",
    "            log_prob = model(actions=data, compute_log_prob=True).detach()\n",
    "            total_log_prob -= log_prob\n",
    "            if printer is not None:\n",
    "                prob = log_prob.detach().exp()\n",
    "                printer.update(epoch + 1, - log_prob / batch_size, prob / batch_size)\n",
    "    return total_log_prob / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5eb40f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process steps 1 / 16353\n",
      "process steps 101 / 16353\n",
      "process steps 201 / 16353\n",
      "process steps 301 / 16353\n",
      "process steps 401 / 16353\n",
      "process steps 501 / 16353\n",
      "process steps 601 / 16353\n",
      "process steps 701 / 16353\n",
      "process steps 801 / 16353\n",
      "process steps 901 / 16353\n",
      "process steps 1001 / 16353\n",
      "process steps 1101 / 16353\n",
      "process steps 1201 / 16353\n",
      "process steps 1301 / 16353\n",
      "process steps 1401 / 16353\n",
      "process steps 1501 / 16353\n",
      "process steps 1601 / 16353\n",
      "process steps 1701 / 16353\n",
      "process steps 1801 / 16353\n",
      "process steps 1901 / 16353\n",
      "process steps 2001 / 16353\n",
      "process steps 2101 / 16353\n",
      "process steps 2201 / 16353\n",
      "process steps 2301 / 16353\n",
      "process steps 2401 / 16353\n",
      "process steps 2501 / 16353\n",
      "process steps 2601 / 16353\n",
      "process steps 2701 / 16353\n",
      "process steps 2801 / 16353\n",
      "process steps 2901 / 16353\n",
      "process steps 3001 / 16353\n",
      "process steps 3101 / 16353\n",
      "process steps 3201 / 16353\n",
      "process steps 3301 / 16353\n",
      "process steps 3401 / 16353\n",
      "process steps 3501 / 16353\n",
      "process steps 3601 / 16353\n",
      "process steps 3701 / 16353\n",
      "process steps 3801 / 16353\n",
      "process steps 3901 / 16353\n",
      "process steps 4001 / 16353\n",
      "process steps 4101 / 16353\n",
      "process steps 4201 / 16353\n",
      "process steps 4301 / 16353\n",
      "process steps 4401 / 16353\n",
      "process steps 4501 / 16353\n",
      "process steps 4601 / 16353\n",
      "process steps 4701 / 16353\n",
      "process steps 4801 / 16353\n",
      "process steps 4901 / 16353\n",
      "process steps 5001 / 16353\n",
      "process steps 5101 / 16353\n",
      "process steps 5201 / 16353\n",
      "process steps 5301 / 16353\n",
      "process steps 5401 / 16353\n",
      "process steps 5501 / 16353\n",
      "process steps 5601 / 16353\n",
      "process steps 5701 / 16353\n",
      "process steps 5801 / 16353\n",
      "process steps 5901 / 16353\n",
      "process steps 6001 / 16353\n",
      "process steps 6101 / 16353\n",
      "process steps 6201 / 16353\n",
      "process steps 6301 / 16353\n",
      "process steps 6401 / 16353\n",
      "process steps 6501 / 16353\n",
      "process steps 6601 / 16353\n",
      "process steps 6701 / 16353\n",
      "process steps 6801 / 16353\n",
      "process steps 6901 / 16353\n",
      "process steps 7001 / 16353\n",
      "process steps 7101 / 16353\n",
      "process steps 7201 / 16353\n",
      "process steps 7301 / 16353\n",
      "process steps 7401 / 16353\n",
      "process steps 7501 / 16353\n",
      "process steps 7601 / 16353\n",
      "process steps 7701 / 16353\n",
      "process steps 7801 / 16353\n",
      "process steps 7901 / 16353\n",
      "process steps 8001 / 16353\n",
      "process steps 8101 / 16353\n",
      "process steps 8201 / 16353\n",
      "process steps 8301 / 16353\n",
      "process steps 8401 / 16353\n",
      "process steps 8501 / 16353\n",
      "process steps 8601 / 16353\n",
      "process steps 8701 / 16353\n",
      "process steps 8801 / 16353\n",
      "process steps 8901 / 16353\n",
      "process steps 9001 / 16353\n",
      "process steps 9101 / 16353\n",
      "process steps 9201 / 16353\n",
      "process steps 9301 / 16353\n",
      "process steps 9401 / 16353\n",
      "process steps 9501 / 16353\n",
      "process steps 9601 / 16353\n",
      "process steps 9701 / 16353\n",
      "process steps 9801 / 16353\n",
      "process steps 9901 / 16353\n",
      "process steps 10001 / 16353\n",
      "process steps 10101 / 16353\n",
      "process steps 10201 / 16353\n",
      "process steps 10301 / 16353\n",
      "process steps 10401 / 16353\n",
      "process steps 10501 / 16353\n",
      "process steps 10601 / 16353\n",
      "process steps 10701 / 16353\n",
      "process steps 10801 / 16353\n",
      "process steps 10901 / 16353\n",
      "process steps 11001 / 16353\n",
      "process steps 11101 / 16353\n",
      "process steps 11201 / 16353\n",
      "process steps 11301 / 16353\n",
      "process steps 11401 / 16353\n",
      "process steps 11501 / 16353\n",
      "process steps 11601 / 16353\n",
      "process steps 11701 / 16353\n",
      "process steps 11801 / 16353\n",
      "process steps 11901 / 16353\n",
      "process steps 12001 / 16353\n",
      "process steps 12101 / 16353\n",
      "process steps 12201 / 16353\n",
      "process steps 12301 / 16353\n",
      "process steps 12401 / 16353\n",
      "process steps 12501 / 16353\n",
      "process steps 12601 / 16353\n",
      "process steps 12701 / 16353\n",
      "process steps 12801 / 16353\n",
      "process steps 12901 / 16353\n",
      "process steps 13001 / 16353\n",
      "process steps 13101 / 16353\n",
      "process steps 13201 / 16353\n",
      "process steps 13301 / 16353\n",
      "process steps 13401 / 16353\n",
      "process steps 13501 / 16353\n",
      "process steps 13601 / 16353\n",
      "process steps 13701 / 16353\n",
      "process steps 13801 / 16353\n",
      "process steps 13901 / 16353\n",
      "process steps 14001 / 16353\n",
      "process steps 14101 / 16353\n",
      "process steps 14201 / 16353\n",
      "process steps 14301 / 16353\n",
      "process steps 14401 / 16353\n",
      "process steps 14501 / 16353\n",
      "process steps 14601 / 16353\n",
      "process steps 14701 / 16353\n",
      "process steps 14801 / 16353\n",
      "process steps 14901 / 16353\n",
      "process steps 15001 / 16353\n",
      "process steps 15101 / 16353\n",
      "process steps 15201 / 16353\n",
      "process steps 15301 / 16353\n",
      "process steps 15401 / 16353\n",
      "process steps 15501 / 16353\n",
      "process steps 15601 / 16353\n",
      "process steps 15701 / 16353\n",
      "process steps 15801 / 16353\n",
      "process steps 15901 / 16353\n",
      "process steps 16001 / 16353\n",
      "process steps 16101 / 16353\n",
      "process steps 16201 / 16353\n",
      "process steps 16301 / 16353\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        log_prob = model(actions=data, compute_log_prob=True)\n",
    "        prob = log_prob.detach().exp()\n",
    "        \n",
    "        loss_averaged = - log_prob\n",
    "        prob_averaged = prob\n",
    "        optimizer.backward_and_step(loss_averaged)\n",
    "        \n",
    "        if (i % 100) == 0:\n",
    "            print('process steps {} / {}'.format(i+1, len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bcd02a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(99.3213, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "612a7afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.2868e-44)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a15ad887",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_log_prob = evaluate(0, model, val_loader, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8f4446c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(90.0992)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f8dd7cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.419861355615263e-40"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(- val_log_prob).exp().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3aa8d3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model_state_dict': model.state_dict()}, args['checkpoint_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8248ce",
   "metadata": {},
   "source": [
    "## multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7efa3da6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b206f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
